{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "OOZlIL1qDzTW"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import spacy\n",
    "from gensim import corpora\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "X9rqlY1-YveZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28712, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_dataset_train_cnn= pd.read_csv(\"dataset_train_CNN\")\n",
    "pre_dataset_train_cnn.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Dr8cQuEpeUEk"
   },
   "outputs": [],
   "source": [
    "reduced_data = pre_dataset_train_cnn.sample(frac=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RChYATDbfQjs",
    "outputId": "79446511-f36b-40e4-f409-761b26600952"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14356, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences related to topic 3\n",
      "- Few options: Sundby (taking part in an 800-mile .\n",
      "- 'I am a determined fighter and extremely .\n",
      "- affordable, lifesaving medical insurance policy has been canceled .\n",
      "- Think Progress argued that United selfishly 'packed .\n",
      "- USA Today oped editor David Mastio took issue with that assessment.\n",
      "- president's promise, \"You can keep your health plan\"?\n",
      "- that \"You can keep your doctor\"?'\n",
      "- He called Pfeiffer a 'fast-talking flack' and Think Progress – the house blog of a group run by former Clinton White House chief of staff John Podesta – 'a leftist propaganda outfit.'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim import corpora, models\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sentences = sent_tokenize(reduced_data.iloc[0]['article'])\n",
    "\n",
    "# Tokenize sentences \n",
    "tokenized_sentences = [sentence.lower().split() for sentence in sentences]\n",
    "\n",
    "# Create dictionary and corpus for LDA\n",
    "dictionary = corpora.Dictionary(tokenized_sentences)\n",
    "corpus = [dictionary.doc2bow(tokens) for tokens in tokenized_sentences]\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = models.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=10)\n",
    "\n",
    "# Function to get dominant topic of a sentence\n",
    "def get_dominant_topic(sentence):\n",
    "    bow = dictionary.doc2bow(sentence.lower().split())\n",
    "    topic_distribution = lda_model.get_document_topics(bow)\n",
    "    dominant_topic = max(topic_distribution, key=lambda x: x[1])[0]\n",
    "    return dominant_topic\n",
    "\n",
    "# Target dominant topic (example)\n",
    "target_topic = 3\n",
    "\n",
    "# Function to filter sentences based on topic similarity\n",
    "def filter_sentences(sentences, target_topic):\n",
    "    filtered_sentences = []\n",
    "    for sentence in sentences:\n",
    "        dominant_topic = get_dominant_topic(sentence)\n",
    "        if dominant_topic == target_topic:\n",
    "            filtered_sentences.append(sentence)\n",
    "    return filtered_sentences\n",
    "\n",
    "# Filter sentences based on the target dominant topic\n",
    "filtered_sentences = filter_sentences(sentences, target_topic)\n",
    "\n",
    "# Print the filtered sentences\n",
    "print(\"Sentences related to topic\", target_topic)\n",
    "for sentence in filtered_sentences:\n",
    "    print(\"-\", sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 8\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences), len(filtered_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Topic 1': 'the her and a that for as plan she of',\n",
      " 'Topic 2': 'the that her to . is cancer a in and',\n",
      " 'Topic 3': 'to a the . with and insurance of it an',\n",
      " 'Topic 4': '. that a of can \"you your keep think progress',\n",
      " 'Topic 5': 'the to and a of insurance . have is my'}\n"
     ]
    }
   ],
   "source": [
    "topic_names = {}\n",
    "\n",
    "for i, topic in lda_model.show_topics(formatted=False):\n",
    "    top_words = [word for word, _ in topic]\n",
    "    topic_name = \" \".join(top_words)\n",
    "    topic_names[f\"Topic {i+1}\"] = topic_name\n",
    "\n",
    "\n",
    "pprint(topic_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token.isalnum() and token not in stop_words]\n",
    "\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['plan'],\n",
       " ['cancer'],\n",
       " ['insurance'],\n",
       " ['keep', 'think', 'progress'],\n",
       " ['insurance']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_topics = [preprocess(doc) for doc in topic_names.values()]\n",
    "preprocessed_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
